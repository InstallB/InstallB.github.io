<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Video Dataset Distillation</title>
</head>
<body>
    <h2>Video Dataset Distillation: Enhancing Diversity and Realism in Video Data for Efficient Training</h2>
    This is the website for SJTU Course CS348: Computer Vision (2024 Fall) Team H's Project.

    <h3>Authors</h3>
    Haoran Wang, Ruohong Jiang, Yifan Chen, from Shanghai Jiao Tong University
    <h3>Paper</h3>
    <p> Our paper is available <a href="https://github.com/InstallB/InstallB.github.io/paper.pdf"> here</a>.</p>
    <h3>Source code</h3>
    <p> Our project is available at <a href="https://github.com/InstallB/video_distillation"> https://github.com/InstallB/video_distillation</a>.</p>

    <h2>Abstract</h2>
    <p> Training large neural networks on video datasets presents substantial computational challenges, especially when
dealing with high-resolution and large-scale data. Inspired by recent advances in dataset distillation, we propose
a novel approach to video data distillation, aiming to compress and optimize video datasets for efficient training
while preserving critical characteristics such as realism, diversity, and efficiency. Our method demonstrates
significant improvements in both accuracy and computational efficiency, achieving comparable performance to
models trained on full datasets, but with drastically reduced computational cost. We conduct extensive empirical
evaluations across various video datasets and model architectures, showing that our approach can effectively distill
large video datasets into compact yet high-fidelity subsets, maintaining key properties crucial for real-world video
applications.</p>


    <h2>Our pipeline</h2>
    <img src="image/pipeline.png" alt="Pipeline">
    <h2>Example result</h2>
    <img src="image/example.gif" alt="Example Distillation for 'Basketball' label in UCF101">

</body>
</html>